{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating model energy\n",
    "\n",
    "[![Open In\n",
    "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nengo/keras-spiking/blob/master/docs/examples/model-energy.ipynb)\n",
    "\n",
    "One of the main motivations for using spiking methods is the potential for significant\n",
    "energy savings over standard techniques. Thus it is useful to be able to estimate how\n",
    "much energy would be used by a model on different devices, so that we can get an\n",
    "idea of how different model/device parameters affect the energy usage before pursuing a\n",
    "full deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:23:40.951581Z",
     "iopub.status.busy": "2023-02-08T15:23:40.951117Z",
     "iopub.status.idle": "2023-02-08T15:23:42.660014Z",
     "shell.execute_reply": "2023-02-08T15:23:42.659186Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 15:23:41.084457: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras_spiking\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "tf.get_logger().addFilter(lambda rec: \"Tracing is expensive\" not in rec.msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "It is important to keep in mind that actual power usage will be heavily dependent on the\n",
    "specific details of the underlying software and hardware implementation. The numbers\n",
    "provided by KerasSpiking should be taken as very rough estimates only, and they rely on\n",
    "a number of assumptions:\n",
    "\n",
    "- **Device specifications**: In order to estimate the energy used by a model on a\n",
    "particular device, we need to know how much energy is used per synaptic operation/neuron\n",
    "update. We rely on published data for these numbers (see our sources for\n",
    "[CPU/GPU/ARM](https://ieeexplore.ieee.org/abstract/document/7054508),\n",
    "[Loihi](https://redwood.berkeley.edu/wp-content/uploads/2021/08/Davies2018.pdf),\n",
    "and [SpiNNaker 1/2](https://arxiv.org/abs/1903.08941)). Energy numbers in practice can\n",
    "differ significantly from published results.\n",
    "- **Overhead**: We do not account for any overhead in the energy estimates (e.g., the\n",
    "cost of transferring data on and off a device). We only estimate the energy usage of\n",
    "internal model computations (synaptic operations and neuron updates). In practice,\n",
    "overhead can be a significant contributor to the energy usage of a model.\n",
    "- **Spiking implementation**: When estimating the energy usage for spiking devices,\n",
    "such as Loihi and Spinnaker, we assume that the model being estimated can be fully\n",
    "converted to a spiking implementation for deployment on the device\n",
    "(even if the input model has non-spiking elements). For example, if the model\n",
    "contains ``tf.keras.layers.Activation(\"relu\")`` layers (non-spiking), we assume that on\n",
    "a spiking device those layers will be converted to something equivalent to\n",
    "``keras_spiking.SpikingActivation(\"relu\")``, and that any connecting layers (e.g.\n",
    "``tf.keras.layers.Dense``) are applied in an event-based fashion (i.e., processing only\n",
    "occurs when the input layer emits a spike). In practice, it is not trivial to map a\n",
    "neural network to a spiking device in this way, and implementation details can\n",
    "significantly affect energy usage. [Nengo](https://www.nengo.ai/nengo/) and\n",
    "[NengoDL](https://www.nengo.ai/nengo-dl/) are designed to make this easier.\n",
    "\n",
    "On non-spiking devices, such as CPU and GPU, we assume that the network runs as a\n",
    "traditional (non-spiking) ANN, and is able to compute the output without iterating\n",
    "over time using non-spiking neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ModelEnergy\n",
    "\n",
    "The ``keras_spiking.ModelEnergy`` class provides the entry point for energy estimation.\n",
    "It takes a Keras model as input, and computes relevant statistics for that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:23:42.663227Z",
     "iopub.status.busy": "2023-02-08T15:23:42.662824Z",
     "iopub.status.idle": "2023-02-08T15:23:45.117767Z",
     "shell.execute_reply": "2023-02-08T15:23:45.117037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Layer (type)                Output Shape              Param #   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d (Conv2D)             (None, 22, 22, 2)         100       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " re_lu (ReLU)                (None, 22, 22, 2)         0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " flatten (Flatten)           (None, 968)               0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense (Dense)               (None, 128)               124032    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " re_lu_1 (ReLU)              (None, 128)               0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_1 (Dense)             (None, 10)                1290      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 125,422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 125,422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-trainable params: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 15:23:44.349161: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-08 15:23:44.913210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10784 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0001:00:00.0, compute capability: 3.7\n"
     ]
    }
   ],
   "source": [
    "# build an example model\n",
    "inp = x = tf.keras.Input((28, 28, 1))\n",
    "x = tf.keras.layers.Conv2D(filters=2, kernel_size=(7, 7))(x)\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(units=128)(x)\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.Dense(units=10)(x)\n",
    "\n",
    "model = tf.keras.Model(inp, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:23:45.120454Z",
     "iopub.status.busy": "2023-02-08T15:23:45.119960Z",
     "iopub.status.idle": "2023-02-08T15:23:45.125471Z",
     "shell.execute_reply": "2023-02-08T15:23:45.124837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)        |Output shape       |Param #|Conn #|Neuron #|J/inf (cpu)\n",
      "--------------------|-------------------|-------|------|--------|-----------\n",
      "input_1 (InputLayer)|[(None, 28, 28, 1)]|      0|     0|       0|          0\n",
      "conv2d (Conv2D)     |  (None, 22, 22, 2)|    100| 47432|       0|    0.00041\n",
      "re_lu (ReLU)        |  (None, 22, 22, 2)|      0|     0|     968|    8.3e-06\n",
      "flatten (Flatten)   |        (None, 968)|      0|     0|       0|          0\n",
      "dense (Dense)       |        (None, 128)| 124032|123904|       0|     0.0011\n",
      "re_lu_1 (ReLU)      |        (None, 128)|      0|     0|     128|    1.1e-06\n",
      "dense_1 (Dense)     |         (None, 10)|   1290|  1280|       0|    1.1e-05\n",
      "============================================================================\n",
      "Total energy per inference [Joules/inf] (cpu): 1.49e-03\n"
     ]
    }
   ],
   "source": [
    "# estimate model energy\n",
    "energy = keras_spiking.ModelEnergy(model)\n",
    "energy.summary(print_warnings=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three columns show the layer name/type, the output shape,\n",
    "and the number of parameters in each layer, and are identical to\n",
    "the corresponding columns in ``model.summary()``.\n",
    "\n",
    "The next column shows the number of connections;\n",
    "two units are connected if a change in the input unit's value\n",
    "changes the output unit's value (assuming non-zero parameters).\n",
    "In a dense connection, the number of connections is the input size\n",
    "times the output size (since each output unit is connected to each input unit);\n",
    "in a convolutional connection, it equals the kernel size times\n",
    "the number of input filters times the output shape.\n",
    "Note that the number of connections can be quite different than the number of\n",
    "parameters, particularly for layers like ``Conv2D`` where parameters are\n",
    "shared between many connections.\n",
    "\n",
    "The next column shows the number of neurons in a layer;\n",
    "for activation layers, this equals the number of output units\n",
    "(i.e. the output size), otherwise it is zero.\n",
    "\n",
    "The last column shows the estimated energy consumption in\n",
    "Joules per inference on a CPU (specifically an Intel i7-4960X).\n",
    "All comparisons made by ``ModelEnergy`` are done using energy per inference,\n",
    "to account for the fact that spiking devices must iterate over a\n",
    "number of timesteps to get an accurate inference,\n",
    "whereas non-spiking devices (such as the CPU here) do not require such iteration.\n",
    "This number represents a lower bound on the amount of energy that might be\n",
    "used by a CPU, since it does not include any overhead,\n",
    "such as energy required to get data on and off the device.\n",
    "\n",
    "We can customize the summary by specifying the columns we want displayed (see [the\n",
    "documentation](https://www.nengo.ai/keras-spiking/reference.html#keras_spiking.ModelEnergy.summary)\n",
    "for the available options, and\n",
    "[here](https://www.nengo.ai/keras-spiking/reference.html#keras_spiking.ModelEnergy)\n",
    "for the built-in devices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:23:45.127825Z",
     "iopub.status.busy": "2023-02-08T15:23:45.127455Z",
     "iopub.status.idle": "2023-02-08T15:23:45.132204Z",
     "shell.execute_reply": "2023-02-08T15:23:45.131569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)    |J/inf (cpu)|J/inf (gpu)|Synop J/inf (|Synop J/inf (|Neuron J/inf (|Neuron J/inf (\n",
      "----------------|-----------|-----------|-------------|-------------|--------------|--------------\n",
      "input_1 (InputLa|          0|          0|            0|            0|             0|             0\n",
      "conv2d (Conv2D) |    0.00041|    1.4e-05|      0.00041|      1.4e-05|             0|             0\n",
      "re_lu (ReLU)    |    8.3e-06|    2.9e-07|            0|            0|       8.3e-06|       2.9e-07\n",
      "flatten (Flatten|          0|          0|            0|            0|             0|             0\n",
      "dense (Dense)   |     0.0011|    3.7e-05|       0.0011|      3.7e-05|             0|             0\n",
      "re_lu_1 (ReLU)  |    1.1e-06|    3.8e-08|            0|            0|       1.1e-06|       3.8e-08\n",
      "dense_1 (Dense) |    1.1e-05|    3.8e-07|      1.1e-05|      3.8e-07|             0|             0\n",
      "==================================================================================================\n",
      "Total energy per inference [Joules/inf] (cpu): 1.49e-03\n",
      "Total energy per inference [Joules/inf] (gpu): 5.21e-05\n"
     ]
    }
   ],
   "source": [
    "energy.summary(\n",
    "    columns=(\n",
    "        \"name\",\n",
    "        \"energy cpu\",\n",
    "        \"energy gpu\",\n",
    "        \"synop_energy cpu\",\n",
    "        \"synop_energy gpu\",\n",
    "        \"neuron_energy cpu\",\n",
    "        \"neuron_energy gpu\",\n",
    "    ),\n",
    "    print_warnings=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see the individual components contributing to the energy usage\n",
    "on each device. The energy spent on synops (short for \"synaptic operations\")\n",
    "is used to multiply values by connection weights;\n",
    "on non-spiking hardware, this has to be done for all connections,\n",
    "but on spiking hardware it is only done when a pre-synaptic neuron spikes.\n",
    "The energy spent on neurons is used to compute neural non-linearities;\n",
    "these neuron updates must happen for all neurons, regardless of input.\n",
    "\n",
    "ModelEnergy has one other parameter, ``example_data``. This data will be passed to the\n",
    "model and used to compute the average firing rate of each layer. This is necessary\n",
    "information for estimating the energy usage of spiking devices, as the number of\n",
    "synaptic updates that need to be performed will be proportional to the firing rates (but\n",
    "has no impact on non-spiking devices, as they perform all synaptic updates every\n",
    "timestep regardless)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:23:45.134742Z",
     "iopub.status.busy": "2023-02-08T15:23:45.134368Z",
     "iopub.status.idle": "2023-02-08T15:24:03.896989Z",
     "shell.execute_reply": "2023-02-08T15:24:03.896316Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 15:23:54.469985: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 19s 19s/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)      |Rate [Hz]|Synop J/inf (cp|Synop J/inf (loih|Neuron J/inf (cp|Neuron J/inf (loih\n",
      "------------------|---------|---------------|-----------------|----------------|------------------\n",
      "input_1 (InputLaye|        1|              0|                0|               0|                 0\n",
      "conv2d (Conv2D)   |        1|        0.00041|          1.3e-09|               0|                 0\n",
      "re_lu (ReLU)      |     0.29|              0|                0|         8.3e-06|           7.8e-08\n",
      "flatten (Flatten) |     0.13|              0|                0|               0|                 0\n",
      "dense (Dense)     |     0.13|         0.0011|          4.3e-10|               0|                 0\n",
      "re_lu_1 (ReLU)    |     0.17|              0|                0|         1.1e-06|             1e-08\n",
      "dense_1 (Dense)   |     0.11|        1.1e-05|          3.9e-12|               0|                 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 15:24:03.740340: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-02-08 15:24:03.740656: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-02-08 15:24:03.740679: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2023-02-08 15:24:03.740990: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-02-08 15:24:03.741049: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    }
   ],
   "source": [
    "energy = keras_spiking.ModelEnergy(model, example_data=np.ones((32, 28, 28)))\n",
    "energy.summary(\n",
    "    columns=(\n",
    "        \"name\",\n",
    "        \"rate\",\n",
    "        \"synop_energy cpu\",\n",
    "        \"synop_energy loihi\",\n",
    "        \"neuron_energy cpu\",\n",
    "        \"neuron_energy loihi\",\n",
    "    ),\n",
    "    print_warnings=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that if we increase the magnitude of the input (and thereby increase the\n",
    "firing rate), the energy estimate increases for the spiking device (Loihi), but not the\n",
    "CPU. Note that only the synaptic energy increases, the neuron energy is unaffected\n",
    "(since it is not dependent on input activity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:24:03.899907Z",
     "iopub.status.busy": "2023-02-08T15:24:03.899409Z",
     "iopub.status.idle": "2023-02-08T15:24:03.987614Z",
     "shell.execute_reply": "2023-02-08T15:24:03.986925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 51ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)      |Rate [Hz]|Synop J/inf (cp|Synop J/inf (loih|Neuron J/inf (cp|Neuron J/inf (loih\n",
      "------------------|---------|---------------|-----------------|----------------|------------------\n",
      "input_1 (InputLaye|        5|              0|                0|               0|                 0\n",
      "conv2d (Conv2D)   |        5|        0.00041|          6.4e-09|               0|                 0\n",
      "re_lu (ReLU)      |      1.5|              0|                0|         8.3e-06|           7.8e-08\n",
      "flatten (Flatten) |     0.64|              0|                0|               0|                 0\n",
      "dense (Dense)     |     0.64|         0.0011|          2.2e-09|               0|                 0\n",
      "re_lu_1 (ReLU)    |     0.85|              0|                0|         1.1e-06|             1e-08\n",
      "dense_1 (Dense)   |     0.56|        1.1e-05|          1.9e-11|               0|                 0\n"
     ]
    }
   ],
   "source": [
    "energy = keras_spiking.ModelEnergy(model, example_data=np.ones((32, 28, 28, 1)) * 5)\n",
    "energy.summary(\n",
    "    columns=(\n",
    "        \"name\",\n",
    "        \"rate\",\n",
    "        \"synop_energy cpu\",\n",
    "        \"synop_energy loihi\",\n",
    "        \"neuron_energy cpu\",\n",
    "        \"neuron_energy loihi\",\n",
    "    ),\n",
    "    print_warnings=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding custom devices\n",
    "\n",
    "We can use ``ModelEnergy.register_device`` to add the specification for new devices,\n",
    "thereby allowing ModelEnergy to provide energy estimates for those devices. This\n",
    "function takes four parameters:\n",
    "\n",
    "- ``name``: An identifying name for the device.\n",
    "- ``energy_per_synop``: The energy (in Joules) required for one synaptic update. A\n",
    "synaptic update is the computation that occurs whenever some input is received by a\n",
    "neuron and multiplied by a weight.\n",
    "- ``energy_per_neuron``: The energy (in Joules) required for one neuron update. A neuron\n",
    "update is the computation that occurs in a neuron every timestep (regardless of whether\n",
    "or not it has received some input).\n",
    "- ``spiking``: Whether or not this is a spiking, or event-based, device. That is, do all\n",
    "synaptic updates occur every timestep (i.e. all the output of one layer is communicated\n",
    "to the next layer every timestep), or do synaptic updates only occur when a neuron in\n",
    "the input layer emits a spike?\n",
    "\n",
    "In addition to registering new devices, this can be used to modify the assumptions for\n",
    "existing devices. For example, if you think the ``gpu`` device specs are too optimistic,\n",
    "you could increase the energy estimates and see what effect that has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:24:03.990178Z",
     "iopub.status.busy": "2023-02-08T15:24:03.989894Z",
     "iopub.status.idle": "2023-02-08T15:24:03.994369Z",
     "shell.execute_reply": "2023-02-08T15:24:03.993756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)        |J/inf (gpu)|J/inf (my-gpu)\n",
      "--------------------|-----------|--------------\n",
      "input_1 (InputLayer)|          0|             0\n",
      "conv2d (Conv2D)     |    1.4e-05|       4.7e-05\n",
      "re_lu (ReLU)        |    2.9e-07|       1.9e-06\n",
      "flatten (Flatten)   |          0|             0\n",
      "dense (Dense)       |    3.7e-05|       0.00012\n",
      "re_lu_1 (ReLU)      |    3.8e-08|       2.6e-07\n",
      "dense_1 (Dense)     |    3.8e-07|       1.3e-06\n",
      "===============================================\n",
      "Total energy per inference [Joules/inf] (gpu): 5.21e-05\n",
      "Total energy per inference [Joules/inf] (my-gpu): 1.75e-04\n"
     ]
    }
   ],
   "source": [
    "keras_spiking.ModelEnergy.register_device(\n",
    "    \"my-gpu\", energy_per_synop=1e-9, energy_per_neuron=2e-9, spiking=False\n",
    ")\n",
    "energy.summary(columns=(\"name\", \"energy gpu\", \"energy my-gpu\"), print_warnings=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal processing\n",
    "\n",
    "Whenever we are working with spiking models it is important to think about how time\n",
    "affects the model. For example, often when working with spiking models we need to run\n",
    "them for multiple timesteps in order to get an accurate estimate of the model's output\n",
    "(see [this\n",
    "example](https://www.nengo.ai/keras-spiking/examples/spiking-fashion-mnist.html) for\n",
    "more details). So in order to make a fair comparison between spiking and non-spiking\n",
    "devices (which only need a single timestep to compute their output), we can specify how\n",
    "many timesteps per inference we expect to run on spiking devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:24:03.996830Z",
     "iopub.status.busy": "2023-02-08T15:24:03.996403Z",
     "iopub.status.idle": "2023-02-08T15:24:04.000361Z",
     "shell.execute_reply": "2023-02-08T15:24:03.999749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)        |J/inf (cpu)|J/inf (loihi)\n",
      "--------------------|-----------|-------------\n",
      "input_1 (InputLayer)|          0|            0\n",
      "conv2d (Conv2D)     |    0.00041|      6.4e-08\n",
      "re_lu (ReLU)        |    8.3e-06|      7.8e-07\n",
      "flatten (Flatten)   |          0|            0\n",
      "dense (Dense)       |     0.0011|      2.2e-08\n",
      "re_lu_1 (ReLU)      |    1.1e-06|        1e-07\n",
      "dense_1 (Dense)     |    1.1e-05|      1.9e-10\n",
      "==============================================\n",
      "Total energy per inference [Joules/inf] (cpu): 1.49e-03\n",
      "Total energy per inference [Joules/inf] (loihi): 9.74e-07\n"
     ]
    }
   ],
   "source": [
    "energy.summary(\n",
    "    columns=(\"name\", \"energy cpu\", \"energy loihi\"),\n",
    "    timesteps_per_inference=10,\n",
    "    print_warnings=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if we use more timesteps per inference that increases the energy estimate for\n",
    "the spiking device, but not the non-spiking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:24:04.002798Z",
     "iopub.status.busy": "2023-02-08T15:24:04.002373Z",
     "iopub.status.idle": "2023-02-08T15:24:04.006445Z",
     "shell.execute_reply": "2023-02-08T15:24:04.005720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)        |J/inf (cpu)|J/inf (loihi)\n",
      "--------------------|-----------|-------------\n",
      "input_1 (InputLayer)|          0|            0\n",
      "conv2d (Conv2D)     |    0.00041|      1.3e-07\n",
      "re_lu (ReLU)        |    8.3e-06|      1.6e-06\n",
      "flatten (Flatten)   |          0|            0\n",
      "dense (Dense)       |     0.0011|      4.3e-08\n",
      "re_lu_1 (ReLU)      |    1.1e-06|      2.1e-07\n",
      "dense_1 (Dense)     |    1.1e-05|      3.9e-10\n",
      "==============================================\n",
      "Total energy per inference [Joules/inf] (cpu): 1.49e-03\n",
      "Total energy per inference [Joules/inf] (loihi): 1.95e-06\n"
     ]
    }
   ],
   "source": [
    "energy.summary(\n",
    "    columns=(\"name\", \"energy cpu\", \"energy loihi\"),\n",
    "    timesteps_per_inference=20,\n",
    "    print_warnings=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to consider the simulation timestep, ``dt``, being used in each of those\n",
    "inference timesteps. This will affect the number of spike events observed, since longer\n",
    "timesteps will result in more spikes (the number of spikes is proportional to\n",
    "``firing_rate*timesteps_per_inference*dt``). Note that the ``dt`` used on the device\n",
    "could be different than the ``dt`` used when training/running the model in KerasSpiking.\n",
    "However, it will default to the same value as ``keras_spiking.default.dt``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:24:04.009730Z",
     "iopub.status.busy": "2023-02-08T15:24:04.009355Z",
     "iopub.status.idle": "2023-02-08T15:24:04.013333Z",
     "shell.execute_reply": "2023-02-08T15:24:04.012685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)        |J/inf (cpu)|J/inf (loihi)\n",
      "--------------------|-----------|-------------\n",
      "input_1 (InputLayer)|          0|            0\n",
      "conv2d (Conv2D)     |    0.00041|      6.4e-09\n",
      "re_lu (ReLU)        |    8.3e-06|      7.8e-08\n",
      "flatten (Flatten)   |          0|            0\n",
      "dense (Dense)       |     0.0011|      2.2e-09\n",
      "re_lu_1 (ReLU)      |    1.1e-06|        1e-08\n",
      "dense_1 (Dense)     |    1.1e-05|      1.9e-11\n",
      "==============================================\n",
      "Total energy per inference [Joules/inf] (cpu): 1.49e-03\n",
      "Total energy per inference [Joules/inf] (loihi): 9.74e-08\n"
     ]
    }
   ],
   "source": [
    "energy.summary(\n",
    "    columns=(\"name\", \"energy cpu\", \"energy loihi\"), dt=0.001, print_warnings=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:24:04.015726Z",
     "iopub.status.busy": "2023-02-08T15:24:04.015345Z",
     "iopub.status.idle": "2023-02-08T15:24:04.019388Z",
     "shell.execute_reply": "2023-02-08T15:24:04.018732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)        |J/inf (cpu)|J/inf (loihi)\n",
      "--------------------|-----------|-------------\n",
      "input_1 (InputLayer)|          0|            0\n",
      "conv2d (Conv2D)     |    0.00041|      1.3e-08\n",
      "re_lu (ReLU)        |    8.3e-06|      7.8e-08\n",
      "flatten (Flatten)   |          0|            0\n",
      "dense (Dense)       |     0.0011|      4.3e-09\n",
      "re_lu_1 (ReLU)      |    1.1e-06|        1e-08\n",
      "dense_1 (Dense)     |    1.1e-05|      3.9e-11\n",
      "==============================================\n",
      "Total energy per inference [Joules/inf] (cpu): 1.49e-03\n",
      "Total energy per inference [Joules/inf] (loihi): 1.06e-07\n"
     ]
    }
   ],
   "source": [
    "energy.summary(\n",
    "    columns=(\"name\", \"energy cpu\", \"energy loihi\"), dt=0.002, print_warnings=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that increasing ``dt`` increases the energy estimate on the spiking device,\n",
    "but not the non-spiking (since the output of a non-spiking neuron is not affected by\n",
    "``dt``). Note that increasing ``dt`` is not exactly equivalent to increasing\n",
    "``timesteps_per_inference``, because ``dt`` only increases the number of synaptic\n",
    "updates, it leaves the number of neuron updates unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final factor to keep in mind regarding temporal models is how time is represented in\n",
    "the Keras model itself. The above models did not have a temporal component, they were\n",
    "simply a single-step feedforward model. ModelEnergy assumes that a non-temporal model\n",
    "represents the computations that will be performed each timestep on a spiking device.\n",
    "But we can also directly define a Keras model that operates over time, which gives us\n",
    "more control over how time is represented. For example, this is equivalent to our\n",
    "original model definition above, but we have added a time dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:24:04.021917Z",
     "iopub.status.busy": "2023-02-08T15:24:04.021538Z",
     "iopub.status.idle": "2023-02-08T15:24:04.108250Z",
     "shell.execute_reply": "2023-02-08T15:24:04.107616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Layer (type)                Output Shape              Param #   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input_2 (InputLayer)        [(None, None, 28, 28, 1)  0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             ]                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " time_distributed (TimeDistr  (None, None, 22, 22, 2)  100       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ibuted)                                                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " re_lu_2 (ReLU)              (None, None, 22, 22, 2)   0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " time_distributed_1 (TimeDis  (None, None, 968)        0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tributed)                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_2 (Dense)             (None, None, 128)         124032    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " re_lu_3 (ReLU)              (None, None, 128)         0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_3 (Dense)             (None, None, 10)          1290      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 125,422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 125,422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-trainable params: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# add a new input dimension (None) representing\n",
    "# temporal data of unknown length\n",
    "inp = x = tf.keras.Input((None, 28, 28, 1))\n",
    "# the TimeDistributed wrapper can be used to apply\n",
    "# non-temporal layers to temporal inputs\n",
    "x = tf.keras.layers.TimeDistributed(\n",
    "    tf.keras.layers.Conv2D(filters=2, kernel_size=(7, 7))\n",
    ")(x)\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())(x)\n",
    "# some layers, like Dense, can operate on temporal data\n",
    "# without requiring a TimeDistributed wrapper\n",
    "x = tf.keras.layers.Dense(units=128)(x)\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.Dense(units=10)(x)\n",
    "\n",
    "temporal_model = tf.keras.Model(inp, x)\n",
    "temporal_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the energy estimates of the temporal and non-temporal models we can see\n",
    "that they are the same,\n",
    "because KerasSpiking is automatically assuming that the non-temporal model will be\n",
    "translated into a temporal model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:24:04.110871Z",
     "iopub.status.busy": "2023-02-08T15:24:04.110415Z",
     "iopub.status.idle": "2023-02-08T15:24:04.193695Z",
     "shell.execute_reply": "2023-02-08T15:24:04.193011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)        |J/inf (cpu)|J/inf (loihi)\n",
      "--------------------|-----------|-------------\n",
      "input_1 (InputLayer)|          0|            0\n",
      "conv2d (Conv2D)     |    0.00041|      1.3e-08\n",
      "re_lu (ReLU)        |    8.3e-06|      7.8e-07\n",
      "flatten (Flatten)   |          0|            0\n",
      "dense (Dense)       |     0.0011|      4.3e-09\n",
      "re_lu_1 (ReLU)      |    1.1e-06|        1e-07\n",
      "dense_1 (Dense)     |    1.1e-05|      3.9e-11\n",
      "==============================================\n",
      "Total energy per inference [Joules/inf] (cpu): 1.49e-03\n",
      "Total energy per inference [Joules/inf] (loihi): 9.05e-07\n"
     ]
    }
   ],
   "source": [
    "energy = keras_spiking.ModelEnergy(model, example_data=np.ones((32, 28, 28, 1)))\n",
    "energy.summary(\n",
    "    columns=(\"name\", \"energy cpu\", \"energy loihi\"),\n",
    "    timesteps_per_inference=10,\n",
    "    print_warnings=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:24:04.196324Z",
     "iopub.status.busy": "2023-02-08T15:24:04.196040Z",
     "iopub.status.idle": "2023-02-08T15:24:04.330854Z",
     "shell.execute_reply": "2023-02-08T15:24:04.330162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 79ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)                        |J/inf (cpu)|J/inf (loihi)\n",
      "------------------------------------|-----------|-------------\n",
      "input_2 (InputLayer)                |          0|            0\n",
      "time_distributed (TimeDistributed)  |    0.00041|      1.3e-08\n",
      "re_lu_2 (ReLU)                      |    8.3e-06|      7.8e-07\n",
      "time_distributed_1 (TimeDistributed)|          0|            0\n",
      "dense_2 (Dense)                     |     0.0011|      2.3e-08\n",
      "re_lu_3 (ReLU)                      |    1.1e-06|        1e-07\n",
      "dense_3 (Dense)                     |    1.1e-05|      1.5e-10\n",
      "==============================================================\n",
      "Total energy per inference [Joules/inf] (cpu): 1.49e-03\n",
      "Total energy per inference [Joules/inf] (loihi): 9.24e-07\n"
     ]
    }
   ],
   "source": [
    "# note that we add a temporal dimension to our example data (which does not need to be\n",
    "# the same length as timesteps_per_inference)\n",
    "energy = keras_spiking.ModelEnergy(\n",
    "    temporal_model, example_data=np.ones((32, 5, 28, 28, 1))\n",
    ")\n",
    "energy.summary(\n",
    "    columns=(\"name\", \"energy cpu\", \"energy loihi\"),\n",
    "    timesteps_per_inference=10,\n",
    "    print_warnings=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example the model was assumed to be temporal because it had `None`\n",
    "as the shape of the first (non-batch) axis. However, in some cases the Keras\n",
    "model definition can be ambiguous as to whether it represents a temporal or\n",
    "non-temporal model.\n",
    "\n",
    "For example, consider the following model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:24:04.333518Z",
     "iopub.status.busy": "2023-02-08T15:24:04.333230Z",
     "iopub.status.idle": "2023-02-08T15:24:04.350536Z",
     "shell.execute_reply": "2023-02-08T15:24:04.349882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Layer (type)                Output Shape              Param #   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input_3 (InputLayer)        [(None, 28, 28)]          0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " re_lu_4 (ReLU)              (None, 28, 28)            0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-trainable params: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = tf.keras.Input((28, 28))\n",
    "x = tf.keras.layers.ReLU()(inp)\n",
    "model = tf.keras.Model(inp, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a temporal model, with 28 neurons being applied for 28 timesteps? Or is it a\n",
    "non-temporal model, with 784 neurons being applied to a 28x28 2D input? The definition\n",
    "is ambiguous, so ``ModelEnergy`` will assume that this is a non-temporal model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:24:04.353521Z",
     "iopub.status.busy": "2023-02-08T15:24:04.353237Z",
     "iopub.status.idle": "2023-02-08T15:24:04.357697Z",
     "shell.execute_reply": "2023-02-08T15:24:04.357042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)        |Output shape    |Neuron #|J/inf (cpu)\n",
      "--------------------|----------------|--------|-----------\n",
      "input_3 (InputLayer)|[(None, 28, 28)]|       0|          0\n",
      "re_lu_4 (ReLU)      |  (None, 28, 28)|     784|    6.7e-06\n",
      "==========================================================\n",
      "Total energy per inference [Joules/inf] (cpu): 6.74e-06\n"
     ]
    }
   ],
   "source": [
    "energy = keras_spiking.ModelEnergy(model)\n",
    "energy.summary(\n",
    "    columns=(\"name\", \"output_shape\", \"neurons\", \"energy cpu\"), print_warnings=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can signal to ``ModelEnergy`` that the ReLU layer should be considered temporal by\n",
    "wrapping it in a ``TimeDistributed`` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:24:04.360454Z",
     "iopub.status.busy": "2023-02-08T15:24:04.360037Z",
     "iopub.status.idle": "2023-02-08T15:24:04.380428Z",
     "shell.execute_reply": "2023-02-08T15:24:04.379775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)                        |Output shape    |Neuron #|J/inf (cpu)\n",
      "------------------------------------|----------------|--------|-----------\n",
      "input_4 (InputLayer)                |[(None, 28, 28)]|       0|          0\n",
      "time_distributed_2 (TimeDistributed)|  (None, 28, 28)|      28|    2.4e-07\n",
      "==========================================================================\n",
      "Total energy per inference [Joules/inf] (cpu): 2.41e-07\n"
     ]
    }
   ],
   "source": [
    "inp = tf.keras.Input((28, 28))\n",
    "x = tf.keras.layers.TimeDistributed(tf.keras.layers.ReLU())(inp)\n",
    "model = tf.keras.Model(inp, x)\n",
    "\n",
    "energy = keras_spiking.ModelEnergy(model)\n",
    "energy.summary(\n",
    "    columns=(\"name\", \"output_shape\", \"neurons\", \"energy cpu\"), print_warnings=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could have changed the shape of the first dimension to `None`, in\n",
    "which case ModelEnergy will assume that that dimension represents time, without the\n",
    "need for a TimeDistributed wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:24:04.383206Z",
     "iopub.status.busy": "2023-02-08T15:24:04.382799Z",
     "iopub.status.idle": "2023-02-08T15:24:04.392769Z",
     "shell.execute_reply": "2023-02-08T15:24:04.392111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)        |Output shape      |Neuron #|J/inf (cpu)\n",
      "--------------------|------------------|--------|-----------\n",
      "input_5 (InputLayer)|[(None, None, 28)]|       0|          0\n",
      "re_lu_6 (ReLU)      |  (None, None, 28)|      28|    2.4e-07\n",
      "============================================================\n",
      "Total energy per inference [Joules/inf] (cpu): 2.41e-07\n"
     ]
    }
   ],
   "source": [
    "inp = tf.keras.Input((None, 28))\n",
    "x = tf.keras.layers.ReLU()(inp)\n",
    "model = tf.keras.Model(inp, x)\n",
    "\n",
    "energy = keras_spiking.ModelEnergy(model)\n",
    "energy.summary(\n",
    "    columns=(\"name\", \"output_shape\", \"neurons\", \"energy cpu\"), print_warnings=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SpikingActivation layers\n",
    "\n",
    "You may have noticed above that we have been silencing some warnings. Let's see what\n",
    "those warnings are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:24:04.395296Z",
     "iopub.status.busy": "2023-02-08T15:24:04.394925Z",
     "iopub.status.idle": "2023-02-08T15:24:04.513708Z",
     "shell.execute_reply": "2023-02-08T15:24:04.513035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 53ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)        |Output shape      |J/inf (loihi)\n",
      "--------------------|------------------|-------------\n",
      "input_6 (InputLayer)|[(None, None, 32)]|            0\n",
      "dense_4 (Dense)     |  (None, None, 64)|      5.6e-11\n",
      "re_lu_7 (ReLU)      |  (None, None, 64)|      5.2e-09\n",
      "=====================================================\n",
      "Total energy per inference [Joules/inf] (loihi): 5.24e-09\n",
      "* These are estimates only; see the documentation for a list of the assumptions being made.\n",
      "  https://bit.ly/3c3aKKH\n",
      "* This model contains non-spiking activations that would not actually behave in the manner we\n",
      "  assume in these calculations; we assume these layers will be converted to spiking equivalents.\n",
      "  Consider using `keras_spiking.SpikingActivation` to make this conversion explicit.\n"
     ]
    }
   ],
   "source": [
    "inp = tf.keras.Input((None, 32))\n",
    "x = tf.keras.layers.Dense(units=64)(inp)\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "model = tf.keras.Model(inp, x)\n",
    "\n",
    "energy = keras_spiking.ModelEnergy(model, example_data=np.ones((8, 10, 32)))\n",
    "energy.summary(columns=(\"name\", \"output_shape\", \"energy loihi\"), print_warnings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first warning highlights that these energy estimates are highly dependent on certain\n",
    "assumptions being made (which we [discussed above](#Assumptions)).\n",
    "\n",
    "The second warning is due to the fact that we are estimating energy on a spiking device\n",
    "but our model contains non-spiking activation functions (ReLU). When estimating energy\n",
    "on spiking devices we assume that neurons will be outputting spikes (in order to compute\n",
    "the number of synaptic updates that need to occur). But if we were to directly map this\n",
    "model to a spiking device 1) that may not even be possible, many spiking devices can\n",
    "_only_ simulating spiking neurons, and 2) these neurons would be triggering synaptic\n",
    "updates on every timestep, not at the rates displayed above.\n",
    "\n",
    "In order to provide a useful estimate for spiking devices, we assume that any\n",
    "non-spiking neurons will be converted to spiking neurons when the model is mapped to the\n",
    "device. However, that may not be a safe assumption; it is better to be explicit and\n",
    "directly convert the Keras model to a spiking one using\n",
    "``keras_spiking.SpikingActivation``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:24:04.516558Z",
     "iopub.status.busy": "2023-02-08T15:24:04.516092Z",
     "iopub.status.idle": "2023-02-08T15:24:04.769356Z",
     "shell.execute_reply": "2023-02-08T15:24:04.768676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 54ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)                          |Output shape      |J/inf (loihi)\n",
      "--------------------------------------|------------------|-------------\n",
      "input_7 (InputLayer)                  |[(None, None, 32)]|            0\n",
      "dense_5 (Dense)                       |  (None, None, 64)|      5.6e-11\n",
      "spiking_activation (SpikingActivation)|  (None, None, 64)|      5.2e-09\n",
      "=======================================================================\n",
      "Total energy per inference [Joules/inf] (loihi): 5.24e-09\n",
      "* These are estimates only; see the documentation for a list of the assumptions being made.\n",
      "  https://bit.ly/3c3aKKH\n"
     ]
    }
   ],
   "source": [
    "inp = tf.keras.Input((None, 32))\n",
    "x = tf.keras.layers.Dense(units=64)(inp)\n",
    "x = keras_spiking.SpikingActivation(\"relu\")(x)\n",
    "model = tf.keras.Model(inp, x)\n",
    "\n",
    "energy = keras_spiking.ModelEnergy(model, example_data=np.ones((8, 10, 32)))\n",
    "energy.summary(columns=(\"name\", \"output_shape\", \"energy loihi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying to real devices\n",
    "\n",
    "Once we've gotten an idea what the energy usage might be for our model on different\n",
    "devices, we likely want to actually deploy our model on one of those devices and see how\n",
    "it performs in the real world. For this we can use [Nengo](https://www.nengo.ai), which\n",
    "provides a suite of tools for running neural models on different hardware platforms.\n",
    "\n",
    "For example, suppose we would like to run the above model on Loihi.  First, we can use\n",
    "the NengoDL converter to automatically convert our Keras model to a Nengo model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:24:04.771893Z",
     "iopub.status.busy": "2023-02-08T15:24:04.771610Z",
     "iopub.status.idle": "2023-02-08T15:24:07.432563Z",
     "shell.execute_reply": "2023-02-08T15:24:07.431700Z"
    }
   },
   "outputs": [],
   "source": [
    "# pylint: disable=wrong-import-order\n",
    "\n",
    "import nengo_dl\n",
    "import nengo_loihi\n",
    "\n",
    "converter = nengo_dl.Converter(model, temporal_model=True, inference_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of the Nengo ecosystem is that once we have a Nengo model, we can run that\n",
    "model on any Nengo-supported hardware platform. For example, if we would like to run on\n",
    "Loihi, we just create a `nengo_loihi.Simulator` and run our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-08T15:24:07.436011Z",
     "iopub.status.busy": "2023-02-08T15:24:07.435369Z",
     "iopub.status.idle": "2023-02-08T15:24:07.776570Z",
     "shell.execute_reply": "2023-02-08T15:24:07.774767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64)\n"
     ]
    }
   ],
   "source": [
    "with nengo_loihi.Simulator(converter.net) as sim:\n",
    "    sim.run_steps(10)\n",
    "\n",
    "print(sim.data[converter.outputs[model.output]].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have an actual Loihi board hooked up here this is just running in an\n",
    "emulator, but if we had a physical board attached the code would be the same (and\n",
    "NengoLoihi would automatically use the board). And that's all that would be required to\n",
    "deploy your model to a spiking device, and start seeing how it performs in the real\n",
    "world!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We can use ``ModelEnergy`` to estimate the energy usage of a Keras model on different\n",
    "hardware platforms. We have looked at the various parameters of these estimates (example\n",
    "data, device specifications, the number of timesteps per inference, and the hardware\n",
    "simulation timestep), as well as how we can customize the input Keras model in different\n",
    "ways (adding temporal features or SpikingActivation layers).\n",
    "\n",
    "As we mentioned at the start, it is important to keep in mind that these numbers are\n",
    "only rough estimates; actual energy usage will be heavily dependent on the details of\n",
    "the hardware and software implementation when mapping your model to a physical device.\n",
    "\n",
    "After you have explored different options using ModelEnergy, you will likely want to\n",
    "actually deploy your model on one of these devices to see how it performs in the real\n",
    "world. This is where the Nengo ecosystem can be very helpful, as it allows you to run a\n",
    "neuron model on any Nengo-supported platform (non-spiking devices like standard CPUs and\n",
    "GPUs, or spiking devices like Loihi or SpiNNaker). You can use the [NengoDL\n",
    "Converter](https://www.nengo.ai/nengo-dl/converter.html) to automatically convert a\n",
    "Keras model (including KerasSpiking) to a Nengo network, and then you can use any Nengo\n",
    "backend (e.g. [NengoDL](https://www.nengo.ai/nengo-dl),\n",
    "[NengoOCL](https://labs.nengo.ai/nengo-ocl), or\n",
    "[NengoLoihi](https://www.nengo.ai/nengo-loihi)) to run that network on different\n",
    "hardware platforms. See [this\n",
    "example](https://www.nengo.ai/nengo-loihi/examples/keras-to-loihi.html) for an\n",
    "end-to-end walkthrough of deploying a Keras model to Loihi."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
