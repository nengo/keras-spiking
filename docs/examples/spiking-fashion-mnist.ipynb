{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Fashion MNIST with spiking activations\n",
    "\n",
    "[![Open In\n",
    "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nengo/keras-spiking/blob/master/docs/examples/spiking-fashion-mnist.ipynb)\n",
    "\n",
    "This example is based on the\n",
    "[Basic image classification example in\n",
    "TensorFlow](https://www.tensorflow.org/tutorials/keras/classification).\n",
    "We would recommend beginning there if you would like a more basic introduction to how\n",
    "Keras works. In this example we will walk through how we can convert that non-spiking\n",
    "model into a spiking model using KerasSpiking, and various techniques that can be used\n",
    "to fine tune performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras_spiking\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "We'll begin by loading the Fashion MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    (train_images, train_labels),\n",
    "    (test_images, test_labels),\n",
    ") = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# normalize images so values are between 0 and 1\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "class_names = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(class_names[train_labels[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-spiking model\n",
    "\n",
    "Next we'll build and train the non-spiking model (this is identical to the\n",
    "[original TensorFlow\n",
    "example](https://www.tensorflow.org/tutorials/keras/classification))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def train(input_model, train_x, test_x):\n",
    "    input_model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    input_model.fit(train_x, train_labels, epochs=10)\n",
    "\n",
    "    _, test_acc = input_model.evaluate(test_x, test_labels, verbose=2)\n",
    "\n",
    "    print(\"\\nTest accuracy:\", test_acc)\n",
    "\n",
    "\n",
    "train(model, train_images, test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiking model\n",
    "\n",
    "Next we will create an equivalent spiking model. There are three important changes here:\n",
    "\n",
    "1. Add a temporal dimension to the data/model.\n",
    "\n",
    "Spiking models always run over time (i.e., each forward pass through the model will run\n",
    "for some number of timesteps). This means that we need to add a temporal dimension to\n",
    "the data, so instead of having shape `(batch_size, ...)` it will have shape\n",
    "`(batch_size, n_steps, ...)`. For those familiar with working with RNNs, the principles\n",
    "are the same; a spiking neuron accepts temporal data and computes over time, just like\n",
    "an RNN.\n",
    "\n",
    "2. Replace any activation functions with `keras_spiking.SpikingActivation`.\n",
    "\n",
    "`keras_spiking.SpikingActivation` can encapsulate any activation function, and will\n",
    "produce an equivalent spiking implementation. Neurons will spike at a rate proportional\n",
    "to the output of the base activation function. For example, if the activation function\n",
    "is outputting a value of 10, then the wrapped `SpikingActivation` will output spikes at\n",
    "a rate of 10Hz (i.e., 10 spikes per 1 simulated second, where 1 simulated second is\n",
    "equivalent to some number of timesteps, determined by the `dt` parameter of\n",
    "`SpikingActivation`).\n",
    "\n",
    "Note that for many layers, Keras combines the activation function into another layer.\n",
    "For example, `tf.keras.layers.Dense(units=10, activation=\"relu\")` is equivalent to\n",
    "`tf.keras.layers.Dense(units=10) -> tf.keras.layers.Activation(\"relu\")`. Due to the\n",
    "temporal nature of `SpikingActivation` it cannot be directly used within another layer\n",
    "as in the first case; we need to explicitly separate it into its own layer.\n",
    "\n",
    "3. Pool across time\n",
    "\n",
    "The output of our `keras_spiking.SpikingActivation` layer is also a timeseries. For\n",
    "classification, we need to aggregate that temporal information somehow to generate a\n",
    "final prediction. Averaging the output over time is usually a good approach (but not the\n",
    "only method; we could also, e.g., look at the output on the last timestep or the time to\n",
    "first spike). We add a `tf.keras.layers.GlobalAveragePooling1D` layer to average across\n",
    "the temporal dimension of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the images for n_steps\n",
    "n_steps = 10\n",
    "train_sequences = np.tile(train_images[:, None], (1, n_steps, 1, 1))\n",
    "test_sequences = np.tile(test_images[:, None], (1, n_steps, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spiking_model = tf.keras.Sequential(\n",
    "    [\n",
    "        # add temporal dimension to the input shape; we can set it to None,\n",
    "        # to allow the model to flexibly run for different lengths of time\n",
    "        tf.keras.layers.Reshape((-1, 28 * 28), input_shape=(None, 28, 28)),\n",
    "        # we can use Keras' TimeDistributed wrapper to allow the Dense layer\n",
    "        # to operate on temporal data\n",
    "        tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(128)),\n",
    "        # replace the \"relu\" activation in the non-spiking model with a\n",
    "        # spiking equivalent\n",
    "        keras_spiking.SpikingActivation(\"relu\", spiking_aware_training=False),\n",
    "        # use average pooling layer to average spiking output over time\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# train the model, identically to the non-spiking version,\n",
    "# except using the time sequences as inputs\n",
    "train(spiking_model, train_sequences, test_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while the training accuracy is as good as we expect, the test accuracy\n",
    "is not. This is due to a unique feature of `SpikingActivation`; it will automatically\n",
    "swap the behaviour of the spiking neurons during training. Because spiking neurons are\n",
    "(in general) not differentiable, we cannot directly use the spiking activation function\n",
    "during training. Instead, SpikingActivation will use the base (non-spiking) activation\n",
    "during training, and the spiking version during inference. So during training above we\n",
    "are seeing the performance of the non-spiking model, but during evaluation we are seeing\n",
    "the performance of the spiking model.\n",
    "\n",
    "So the question is, why is the performance of the spiking model so much worse than the\n",
    "non-spiking equivalent, and what can we do to fix that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation time\n",
    "\n",
    "Let's visualize the output of the spiking model, to get a better sense of what is going\n",
    "on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_output(seq_model, modify_dt=None):\n",
    "    \"\"\"\n",
    "    This code is only used for plotting purposes, and isn't necessary to\n",
    "    understand the rest of this example; feel free to skip it\n",
    "    if you just want to see the results.\n",
    "    \"\"\"\n",
    "\n",
    "    # rebuild the model with the functional API, so that we can\n",
    "    # access the output of intermediate layers\n",
    "    inp = x = tf.keras.Input(batch_shape=seq_model.layers[0].input_shape)\n",
    "\n",
    "    has_global_average_pooling = False\n",
    "    for layer in seq_model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.GlobalAveragePooling1D):\n",
    "            # remove the pooling so that we can see the model's\n",
    "            # output over time\n",
    "            has_global_average_pooling = True\n",
    "            continue\n",
    "\n",
    "        if isinstance(layer, (keras_spiking.SpikingActivation, keras_spiking.Lowpass)):\n",
    "            cfg = layer.get_config()\n",
    "            # update dt, if specified\n",
    "            if modify_dt is not None:\n",
    "                cfg[\"dt\"] = modify_dt\n",
    "            # always return the full time series so we can visualize it\n",
    "            cfg[\"return_sequences\"] = True\n",
    "\n",
    "            layer = type(layer)(**cfg)\n",
    "\n",
    "        if isinstance(layer, keras_spiking.SpikingActivation):\n",
    "            # save this layer so we can access it later\n",
    "            spike_layer = layer\n",
    "\n",
    "        x = layer(x)\n",
    "\n",
    "    func_model = tf.keras.Model(inp, [x, spike_layer.output])\n",
    "\n",
    "    # copy weights to new model\n",
    "    func_model.set_weights(seq_model.get_weights())\n",
    "\n",
    "    # run model\n",
    "    output, spikes = func_model.predict(test_sequences)\n",
    "\n",
    "    if has_global_average_pooling:\n",
    "        # check test accuracy using average output over all timesteps\n",
    "        predictions = np.argmax(output.mean(axis=1), axis=-1)\n",
    "    else:\n",
    "        # check test accuracy using output from only the last timestep\n",
    "        predictions = np.argmax(output[:, -1], axis=-1)\n",
    "    accuracy = np.equal(predictions, test_labels).mean()\n",
    "    print(f\"Test accuracy: {100 * accuracy:.2f}%\")\n",
    "\n",
    "    time = test_sequences.shape[1] * spike_layer.dt\n",
    "    n_spikes = spikes * spike_layer.dt\n",
    "    rates = np.sum(n_spikes, axis=1) / time\n",
    "\n",
    "    print(\n",
    "        f\"Spike rate per neuron (Hz): min={np.min(rates):.2f} \"\n",
    "        f\"mean={np.mean(rates):.2f} max={np.max(rates):.2f}\"\n",
    "    )\n",
    "\n",
    "    # plot output\n",
    "    for ii in range(4):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(class_names[test_labels[ii]])\n",
    "        plt.imshow(test_images[ii], cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(\"Spikes per neuron per timestep\")\n",
    "        bin_edges = np.arange(int(np.max(n_spikes[ii])) + 2) - 0.5\n",
    "        plt.hist(np.ravel(n_spikes[ii]), bins=bin_edges)\n",
    "        x_ticks = plt.xticks()[0]\n",
    "        plt.xticks(\n",
    "            x_ticks[(np.abs(x_ticks - np.round(x_ticks)) < 1e-8) & (x_ticks > -1e-8)]\n",
    "        )\n",
    "        plt.xlabel(\"# of spikes\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"Output predictions\")\n",
    "        plt.plot(\n",
    "            np.arange(test_sequences.shape[1]) * spike_layer.dt,\n",
    "            tf.nn.softmax(output[ii]),\n",
    "        )\n",
    "        plt.legend(class_names, loc=\"upper left\")\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Probability\")\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_output(spiking_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an immediate problem: the neurons are hardly spiking at all. The mean number\n",
    "of spikes we're getting out of each neuron in our SpikingActivation layer is much less\n",
    "than one, and as a result the output is mostly flat.\n",
    "\n",
    "To help understand why, we need to think more about the temporal nature of spiking\n",
    "neurons. Recall that the layer is set up such that if the base activation function were\n",
    "to be outputting a value of 1, the spiking equivalent would be spiking at 1Hz (i.e.,\n",
    "emitting one spike per second). In the above example we are simulating for 10 timesteps,\n",
    "with the default `dt` of 0.001s, so we're simulating a total of 0.01s. If our neurons\n",
    "aren't spiking very rapidly, and we're only simulating for 0.01s, then it's not\n",
    "surprising that we aren't getting any spikes in that time window.\n",
    "\n",
    "We can increase the value of `dt`, effectively running the spiking neurons for longer,\n",
    "in order to get a more accurate measure of the neuron's output. Basically this allows us\n",
    "to collect more spikes from each neuron, giving us a better estimate of the neuron's\n",
    "actual spike rate. We can see how the number of spikes and accuracy change as we\n",
    "increase `dt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt=0.01 * 10 timesteps is equivalent to 0.1s of simulated time\n",
    "check_output(spiking_model, modify_dt=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_output(spiking_model, modify_dt=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_output(spiking_model, modify_dt=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as we increase `dt` the performance of the spiking model increasingly\n",
    "approaches the non-spiking performance. In addition, as `dt` increases, the number of\n",
    "spikes is increasing. To understand why this improves accuracy, keep in mind that\n",
    "although the simulated time is increasing, the actual number of timesteps is still 10 in\n",
    "all cases. We're effectively binning all the spikes that occur on each time step. So as\n",
    "our bin sizes get larger (increasing `dt`), the spike counts will more closely\n",
    "approximate the \"true\" output of the underlying non-spiking activation function.\n",
    "\n",
    "One might be tempted to simply increase `dt` to a very large value, and thereby always\n",
    "get great performance. But keep in mind that when we do that we have likely lost any of\n",
    "the advantages that were motivating us to investigate spiking models in the first place.\n",
    "For example, one prominent advantage of spiking models is temporal sparsity (we only\n",
    "need to communicate occasional spikes, rather than continuous values). However, with\n",
    "large `dt` the neurons are likely spiking every simulation time step (or multiple times\n",
    "per timestep), so the activity is no longer temporally sparse.\n",
    "\n",
    "Thus setting `dt` represents a trade-off between accuracy and temporal sparsity.\n",
    "Choosing the appropriate value will depend on the demands of your application.\n",
    "\n",
    "In some cases it can be useful to modify `dt` over the course of training. For example,\n",
    "we could start with a large `dt` and then gradually decrease it over time. See\n",
    "`keras_spiking.callbacks.DtScheduler` for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiking aware training\n",
    "\n",
    "As mentioned above, by default SpikingActivation layers will use the non-spiking\n",
    "activation function during training and the spiking version during inference. However,\n",
    "similar to the idea of\n",
    "[quantization aware\n",
    "training](https://www.tensorflow.org/model_optimization/guide/quantization/training),\n",
    "often we can improve performance by partially incorporating spiking behaviour during\n",
    "training. Specifically, we will use the spiking activation on the forward pass, while\n",
    "still using the non-spiking version on the backwards pass. This allows the model to\n",
    "learn weights that account for the discrete, temporal nature of the spiking activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikeaware_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Reshape((-1, 28 * 28), input_shape=(None, 28, 28)),\n",
    "        tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(128)),\n",
    "        # set spiking_aware training and a moderate dt\n",
    "        keras_spiking.SpikingActivation(\"relu\", dt=0.01, spiking_aware_training=True),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train(spikeaware_model, train_sequences, test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_output(spikeaware_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with `spiking_aware_training` we're getting better performance than we\n",
    "were with the equivalent `dt` value above. The model has learned weights that are less\n",
    "sensitive to the discrete, sparse output produced by the spiking neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spike rate regularization\n",
    "\n",
    "As we saw in the [Simulation time section](#Simulation-time), the spiking rate of the\n",
    "neurons is very important. If a neuron is spiking too slowly then we don't have enough\n",
    "information to determine its output value. Conversely, if a neuron is spiking too\n",
    "quickly then we may lose the spiking advantages we are looking for, such as temporal\n",
    "sparsity.\n",
    "\n",
    "Thus it can be helpful to more directly control the firing rates in the model by\n",
    "applying regularization penalties during training. Any of the standard Keras\n",
    "regularization functions can be used. KerasSpiking also includes some additional\n",
    "regularizers that can be useful for this case as they allow us to specify a non-zero\n",
    "reference point (so we can drive the activities towards some value greater than zero),\n",
    "or a range of acceptable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularized_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Reshape((-1, 28 * 28), input_shape=(None, 28, 28)),\n",
    "        tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(128)),\n",
    "        keras_spiking.SpikingActivation(\n",
    "            \"relu\",\n",
    "            dt=0.01,\n",
    "            spiking_aware_training=True,\n",
    "            # add activity regularizer to encourage spike rates between 10 and 20 Hz\n",
    "            activity_regularizer=keras_spiking.regularizers.L2(\n",
    "                l2=1e-4, target=(10, 20)\n",
    "            ),\n",
    "        ),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train(regularized_model, train_sequences, test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_output(regularized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the spike rates have moved towards the 10-20 Hz target we specified.\n",
    "However, the test accuracy has dropped, since we're adding an additional optimization\n",
    "constraint. (The accuracy is still higher than the original result with `dt=0.01`, due\n",
    "to the higher spike rates.) We could lower the regularization weight to allow more\n",
    "freedom in the firing rates. Or we could use `keras_spiking.regularizers.Percentile`,\n",
    "which allows more freedom for outliers.\n",
    "Again, this is a tradeoff that is made between controlling\n",
    "the firing rates and optimizing accuracy, and the best value for that tradeoff will\n",
    "depend on the particular application (e.g., how important is it that spike rates fall\n",
    "within a particular range?).\n",
    "\n",
    "Note that in some cases it may be better to use regularization with\n",
    "``spiking_aware_training=False``, as the regularization may perform better when the\n",
    "value being regularized is smoother. It may also help to adjust the weight\n",
    "initialization so that the initial firing rates are closer to the desired range, so\n",
    "that there are smaller adjustments required by the regularizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowpass filtering\n",
    "\n",
    "Another tool we can employ when working with SpikingActivation layers is filtering. As\n",
    "we've seen, the output of a spiking layer consists of discrete, temporally sparse spike\n",
    "events. This makes it difficult to determine the spike rate of a neuron when just\n",
    "looking at a single timestep. In the cases above we have worked around this by using a\n",
    "`tf.keras.layers.GlobalAveragePooling1D` layer to average the output across all\n",
    "timesteps before classification.\n",
    "\n",
    "Another way to achieve this is to compute some kind of moving average of the spiking\n",
    "output across timesteps. This is effectively what filtering is doing. KerasSpiking\n",
    "contains a Lowpass layer, which implements a\n",
    "[lowpass filter](https://en.wikipedia.org/wiki/Low-pass_filter). This has a parameter\n",
    "`tau`, known as the filter time constant, which controls the degree of smoothing the\n",
    "layer will apply. Larger `tau` values will apply more smoothing, meaning that we're\n",
    "aggregating information across longer periods of time, but the output will also be\n",
    "slower to adapt to changes in the input.\n",
    "\n",
    "By default the `tau` values are trainable. We can use this in combination with spiking\n",
    "aware training to enable the model to learn time constants that best trade off spike\n",
    "noise versus response speed.\n",
    "\n",
    "Unlike `tf.keras.layers.GlobalAveragePooling1D`, `keras_spiking.Lowpass` computes\n",
    "outputs for all timesteps by default. This makes it possible to apply filtering\n",
    "throughout the model—not only on the final layer—in the case that there are multiple\n",
    "spiking layers. For the final layer, we can pass `return_sequences=False` to have the\n",
    "layer only return the output of the final timestep, rather than the outputs of all\n",
    "timesteps.\n",
    "\n",
    "When working with multiple KerasSpiking layers, we often want them to all share the\n",
    "same `dt`. We can use `keras_spiking.default.dt` to change the default dt for all\n",
    "layers. Note that this will only affect layers created _after_ the default is changed;\n",
    "this will not retroactively affect previous layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_spiking.default.dt = 0.01\n",
    "\n",
    "filtered_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Reshape((-1, 28 * 28), input_shape=(None, 28, 28)),\n",
    "        tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(128)),\n",
    "        keras_spiking.SpikingActivation(\"relu\", spiking_aware_training=True),\n",
    "        # add a lowpass filter on output of spiking layer\n",
    "        keras_spiking.Lowpass(0.1, return_sequences=False),\n",
    "        tf.keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train(filtered_model, train_sequences, test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_output(filtered_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model performs similarly to the previous\n",
    "[spiking aware training](#Spiking-aware-training) example, which makes sense since, for\n",
    "a static input image, a moving average is very similar to a global average. We would\n",
    "need a more complicated model, with multiple spiking layers or inputs that are changing\n",
    "over time, to really see the benefits of a Lowpass layer.\n",
    "The ``keras_spiking.Alpha`` layer is another lowpass-filtering layer,\n",
    "which can provide better filtering of spike noise with less delay\n",
    "than ``keras_spiking.Lowpass``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We can use `SpikingActivation` layers to convert any activation function to an\n",
    "equivalent spiking implementation. Models with SpikingActivations can be trained and\n",
    "evaluated in the same way as non-spiking models, thanks to the swappable\n",
    "training/inference behaviour.\n",
    "\n",
    "There are also a number of additional features that should be kept in mind in order to\n",
    "optimize the performance of a spiking model:\n",
    "\n",
    "- [Simulation time](#Simulation-time): by adjusting `dt` we can trade off temporal\n",
    "  sparsity versus accuracy\n",
    "- [Spiking aware training](#Spiking-aware-training): incorporating spiking dynamics on\n",
    "  the forward pass can allow the model to learn weights that are more robust to spiking\n",
    "  activations\n",
    "- [Spike rate regularization](#Spike-rate-regularization): we can gain more control over\n",
    "  spike rates by directly incorporating activity regularization into the optimization\n",
    "  process\n",
    "- [Lowpass filtering](#Lowpass-filtering): we can achieve better accuracy with fewer\n",
    "  spikes by aggregating spike data over time"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
